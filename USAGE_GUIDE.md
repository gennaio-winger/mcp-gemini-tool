# üìñ Usage Guide - OpenRouter Free-Tier Tracker

Praktische Anleitung mit Beispielen und Workflows f√ºr die Nutzung des Free-Tier Trackers.

---

## üéØ Quick Start

### 1. Ersten Request senden

```javascript
import { updateModelLimits } from './free-tier-tracker.js';

// Nach erfolgreichem API-Call Tracker aktualisieren
const modelId = 'mistralai/mistral-7b-instruct:free';
await updateModelLimits(modelId);

console.log('‚úÖ Request tracked!');
```

### 2. Status pr√ºfen

```bash
node free-tier-tracker.js status
```

### 3. Bestes Modell w√§hlen

```bash
node free-tier-tracker.js best
```

---

## üí° Praktische Beispiele

### Beispiel 1: Einfache Frage

**Task:** Stelle eine einfache Frage an das schnellste Modell.

```javascript
import { getBestAvailableModel, updateModelLimits } from './free-tier-tracker.js';

async function askSimpleQuestion(question) {
  // Schnellstes verf√ºgbares Modell w√§hlen
  const best = await getBestAvailableModel();

  console.log(`Nutze: ${best.name} (${best.remaining}/${best.limit} verf√ºgbar)`);

  // API-Call zu OpenRouter
  const response = await fetch('https://openrouter.ai/api/v1/chat/completions', {
    method: 'POST',
    headers: {
      'Authorization': `Bearer ${process.env.OPENROUTER_API_KEY}`,
      'Content-Type': 'application/json'
    },
    body: JSON.stringify({
      model: best.id,
      messages: [{ role: 'user', content: question }],
      max_tokens: 100
    })
  });

  const data = await response.json();

  // Tracker aktualisieren
  await updateModelLimits(best.id);

  return data.choices[0].message.content;
}

// Nutzen
const answer = await askSimpleQuestion("Was ist 2+2?");
console.log(answer);
```

---

### Beispiel 2: Code-Generierung

**Task:** Generiere Code mit dem spezialisierten Coding-Modell.

```javascript
import { hasRequestsAvailable, updateModelLimits } from './free-tier-tracker.js';

async function generateCode(prompt) {
  const coderModel = 'qwen/qwen3-coder:free';

  // Pr√ºfe ob Requests verf√ºgbar
  const available = await hasRequestsAvailable(coderModel);

  if (!available) {
    console.warn('‚ö†Ô∏è  Qwen Coder hat keine Requests mehr!');
    // Fallback zu anderem Modell
    const best = await getBestAvailableModel();
    console.log(`Nutze stattdessen: ${best.name}`);
    coderModel = best.id;
  }

  // API-Call
  const response = await fetch('https://openrouter.ai/api/v1/chat/completions', {
    method: 'POST',
    headers: {
      'Authorization': `Bearer ${process.env.OPENROUTER_API_KEY}`,
      'Content-Type': 'application/json'
    },
    body: JSON.stringify({
      model: coderModel,
      messages: [
        {
          role: 'system',
          content: 'Du bist ein Code-Experte. Schreibe sauberen, effizienten Code.'
        },
        {
          role: 'user',
          content: prompt
        }
      ],
      max_tokens: 1000,
      temperature: 0.3  // Niedrig f√ºr pr√§zisen Code
    })
  });

  const data = await response.json();

  // Tracker aktualisieren
  await updateModelLimits(coderModel);

  return data.choices[0].message.content;
}

// Nutzen
const code = await generateCode("Schreibe eine Python-Funktion f√ºr Fibonacci");
console.log(code);
```

---

### Beispiel 3: Batch-Processing mit Load-Balancing

**Task:** Verarbeite 100 Fragen und verteile sie auf verf√ºgbare Modelle.

```javascript
import {
  getBestAvailableModel,
  updateModelLimits,
  loadTracker
} from './free-tier-tracker.js';

async function processBatch(questions) {
  const results = [];

  for (let i = 0; i < questions.length; i++) {
    const question = questions[i];

    // W√§hle bestes verf√ºgbares Modell
    const model = await getBestAvailableModel();

    if (model.remaining === 0) {
      console.error('‚ùå Keine Modelle mehr verf√ºgbar! Bitte warten bis Reset.');
      break;
    }

    console.log(`[${i + 1}/${questions.length}] Nutze: ${model.name} (${model.remaining} left)`);

    try {
      // API-Call
      const response = await fetch('https://openrouter.ai/api/v1/chat/completions', {
        method: 'POST',
        headers: {
          'Authorization': `Bearer ${process.env.OPENROUTER_API_KEY}`,
          'Content-Type': 'application/json'
        },
        body: JSON.stringify({
          model: model.id,
          messages: [{ role: 'user', content: question }],
          max_tokens: 200
        })
      });

      const data = await response.json();

      // Tracker aktualisieren
      await updateModelLimits(model.id);

      results.push({
        question,
        answer: data.choices[0].message.content,
        model: model.name
      });

      // Pause zwischen Requests (20 RPM Limit)
      await new Promise(resolve => setTimeout(resolve, 3000)); // 3s Pause

    } catch (error) {
      console.error(`Fehler bei Frage ${i + 1}:`, error.message);
      results.push({ question, error: error.message });
    }
  }

  return results;
}

// Nutzen
const questions = [
  "Was ist KI?",
  "Erkl√§re Quantum Computing",
  "Wie funktioniert Blockchain?",
  // ... 97 weitere Fragen
];

const results = await processBatch(questions);
console.log(`‚úÖ ${results.length} Fragen verarbeitet`);
```

---

### Beispiel 4: Vision-Task (Bild analysieren)

**Task:** Analysiere ein Bild mit dem Vision-spezialisierten Modell.

```javascript
import { updateModelLimits, hasRequestsAvailable } from './free-tier-tracker.js';
import fs from 'fs';

async function analyzeImage(imagePath, question) {
  const visionModel = 'nvidia/nemotron-nano-12b-v2-vl:free';

  // Pr√ºfe Verf√ºgbarkeit
  const available = await hasRequestsAvailable(visionModel);

  if (!available) {
    throw new Error('Nemotron VL hat keine Requests mehr!');
  }

  // Bild als Base64 laden
  const imageBuffer = fs.readFileSync(imagePath);
  const base64Image = imageBuffer.toString('base64');
  const mimeType = 'image/jpeg'; // oder image/png

  // API-Call mit Bild
  const response = await fetch('https://openrouter.ai/api/v1/chat/completions', {
    method: 'POST',
    headers: {
      'Authorization': `Bearer ${process.env.OPENROUTER_API_KEY}`,
      'Content-Type': 'application/json'
    },
    body: JSON.stringify({
      model: visionModel,
      messages: [
        {
          role: 'user',
          content: [
            {
              type: 'text',
              text: question
            },
            {
              type: 'image_url',
              image_url: {
                url: `data:${mimeType};base64,${base64Image}`
              }
            }
          ]
        }
      ],
      max_tokens: 500
    })
  });

  const data = await response.json();

  // Tracker aktualisieren
  await updateModelLimits(visionModel);

  return data.choices[0].message.content;
}

// Nutzen
const analysis = await analyzeImage(
  './screenshot.png',
  'Was ist auf diesem Bild zu sehen?'
);
console.log(analysis);
```

---

### Beispiel 5: Lange Texte mit Grok (2M Context)

**Task:** Analysiere ein sehr langes Dokument mit dem 2M-Context-Modell.

```javascript
import { updateModelLimits } from './free-tier-tracker.js';
import fs from 'fs';

async function analyzeLongDocument(filePath, question) {
  const grokModel = 'x-ai/grok-4.1-fast:free';

  // Lange Datei laden
  const document = fs.readFileSync(filePath, 'utf-8');

  console.log(`Dokument-L√§nge: ${document.length} Zeichen`);
  console.log(`Gesch√§tzte Tokens: ~${Math.floor(document.length / 4)}`);

  // API-Call mit langem Context
  const response = await fetch('https://openrouter.ai/api/v1/chat/completions', {
    method: 'POST',
    headers: {
      'Authorization': `Bearer ${process.env.OPENROUTER_API_KEY}`,
      'Content-Type': 'application/json'
    },
    body: JSON.stringify({
      model: grokModel,
      messages: [
        {
          role: 'user',
          content: `Dokument:\n\n${document}\n\nFrage: ${question}`
        }
      ],
      max_tokens: 2000
    })
  });

  const data = await response.json();

  // Tracker aktualisieren
  await updateModelLimits(grokModel);

  // Token-Usage anzeigen
  if (data.usage) {
    console.log(`üìä Tokens: ${data.usage.prompt_tokens} input + ${data.usage.completion_tokens} output`);
  }

  return data.choices[0].message.content;
}

// Nutzen
const summary = await analyzeLongDocument(
  './long_document.txt',
  'Fasse die Hauptpunkte zusammen'
);
console.log(summary);
```

---

## üéØ Workflows

### Workflow 1: T√§gliche Monitoring-Routine

```bash
#!/bin/bash
# daily-check.sh - Morgens Status pr√ºfen

echo "üìä Free-Tier Status (Morgen-Check)"
echo "=================================="
node free-tier-tracker.js status

echo ""
echo "üèÜ Bestes verf√ºgbares Modell:"
node free-tier-tracker.js best
```

### Workflow 2: Smart Model Selection

```javascript
import { loadTracker, getBestAvailableModel } from './free-tier-tracker.js';

async function selectModelForTask(taskType) {
  const tracker = await loadTracker();

  // Task-spezifische Modell-Auswahl
  const preferences = {
    'coding': ['qwen/qwen3-coder:free', 'mistralai/mistral-7b-instruct:free'],
    'vision': ['nvidia/nemotron-nano-12b-v2-vl:free'],
    'long-context': ['x-ai/grok-4.1-fast:free', 'google/gemini-2.0-flash-exp:free'],
    'fast': ['mistralai/mistral-7b-instruct:free', 'nvidia/nemotron-nano-12b-v2-vl:free'],
    'general': ['meta-llama/llama-3.3-70b-instruct:free', 'google/gemini-2.0-flash-exp:free']
  };

  // Bevorzugte Modelle f√ºr Task-Type
  const preferred = preferences[taskType] || [];

  // Pr√ºfe bevorzugte Modelle
  for (const modelId of preferred) {
    const model = tracker.models[modelId];
    if (model && model.remaining > 0) {
      console.log(`‚úÖ Nutze bevorzugtes Modell: ${model.name}`);
      return { id: modelId, ...model };
    }
  }

  // Fallback: Bestes verf√ºgbares
  console.log('‚ö†Ô∏è  Bevorzugte Modelle nicht verf√ºgbar, w√§hle bestes');
  return await getBestAvailableModel();
}

// Nutzen
const model = await selectModelForTask('coding');
console.log(`Ausgew√§hltes Modell: ${model.name}`);
```

### Workflow 3: Error-Handling mit Retry

```javascript
async function robustRequest(modelId, prompt, maxRetries = 3) {
  let retries = 0;

  while (retries < maxRetries) {
    try {
      // Pr√ºfe Verf√ºgbarkeit
      const available = await hasRequestsAvailable(modelId);

      if (!available) {
        // Wechsle zu anderem Modell
        const best = await getBestAvailableModel();
        if (!best || best.remaining === 0) {
          throw new Error('Keine Modelle verf√ºgbar!');
        }
        modelId = best.id;
        console.log(`Wechsle zu: ${best.name}`);
      }

      // API-Call
      const response = await fetch('https://openrouter.ai/api/v1/chat/completions', {
        method: 'POST',
        headers: {
          'Authorization': `Bearer ${process.env.OPENROUTER_API_KEY}`,
          'Content-Type': 'application/json'
        },
        body: JSON.stringify({
          model: modelId,
          messages: [{ role: 'user', content: prompt }],
          max_tokens: 500
        })
      });

      if (!response.ok) {
        const error = await response.json();

        // Retry bei 429 (Rate-Limit)
        if (response.status === 429) {
          retries++;
          console.warn(`‚ö†Ô∏è  Rate-Limit (429). Retry ${retries}/${maxRetries} in 5s...`);
          await new Promise(resolve => setTimeout(resolve, 5000));
          continue;
        }

        throw new Error(`API-Error ${response.status}: ${error.error?.message}`);
      }

      const data = await response.json();

      // Tracker aktualisieren
      await updateModelLimits(modelId);

      return {
        success: true,
        answer: data.choices[0].message.content,
        model: modelId,
        retries
      };

    } catch (error) {
      retries++;

      if (retries >= maxRetries) {
        return {
          success: false,
          error: error.message,
          retries
        };
      }

      console.error(`Fehler (${retries}/${maxRetries}):`, error.message);
      await new Promise(resolve => setTimeout(resolve, 2000));
    }
  }
}

// Nutzen
const result = await robustRequest('mistralai/mistral-7b-instruct:free', 'Hallo!');
if (result.success) {
  console.log(`‚úÖ Antwort: ${result.answer}`);
} else {
  console.error(`‚ùå Fehler nach ${result.retries} Versuchen: ${result.error}`);
}
```

---

## üõ°Ô∏è Best Practices

### 1. Immer Limits pr√ºfen

```javascript
// ‚úÖ RICHTIG
const available = await hasRequestsAvailable(modelId);
if (!available) {
  const best = await getBestAvailableModel();
  modelId = best.id;
}

// ‚ùå FALSCH (blind senden ohne Pr√ºfung)
await fetch(...)
```

### 2. Rate-Limit beachten (20 RPM)

```javascript
// ‚úÖ RICHTIG (3s Pause zwischen Requests)
for (const question of questions) {
  await askQuestion(question);
  await new Promise(resolve => setTimeout(resolve, 3000));
}

// ‚ùå FALSCH (zu schnell = 429 Error)
await Promise.all(questions.map(q => askQuestion(q)));
```

### 3. Tracker nach JEDEM Request aktualisieren

```javascript
// ‚úÖ RICHTIG
const response = await fetch(...);
await updateModelLimits(modelId);

// ‚ùå FALSCH (vergessen zu tracken)
const response = await fetch(...);
// Tracker wird nicht aktualisiert!
```

### 4. Error-Handling implementieren

```javascript
// ‚úÖ RICHTIG
try {
  await askQuestion(...);
} catch (error) {
  if (error.status === 429) {
    // Warte oder nutze anderes Modell
  }
}

// ‚ùå FALSCH (kein Error-Handling)
await askQuestion(...);
```

---

## üìà Performance-Tipps

### 1. Schnellstes Modell f√ºr einfache Tasks

```javascript
// F√ºr "Was ist 2+2?" ‚Üí Mistral 7B (365ms)
const fast = 'mistralai/mistral-7b-instruct:free';
```

### 2. Max-Tokens begrenzen

```javascript
// ‚úÖ Spart Tokens und Zeit
max_tokens: 100  // F√ºr kurze Antworten

// ‚ùå Verschwenderisch
max_tokens: 4096  // F√ºr "Was ist 2+2?"
```

### 3. Temperature anpassen

```javascript
// Code-Generierung (pr√§zise)
temperature: 0.1

// Kreatives Schreiben
temperature: 0.9

// Standard
temperature: 0.7
```

---

## üîß Debugging

### Debug-Modus aktivieren

```javascript
// In free-tier-tracker.js
console.error(`üêõ Debug: Aktualisiere ${modelId}`);
console.error(`   Remaining: ${model.remaining} ‚Üí ${model.remaining - 1}`);
```

### Tracker-Datei inspizieren

```bash
# Zeige alle Modelle
jq '.models | keys' free-tier-tracker.json

# Zeige Modell-Details
jq '.models["mistralai/mistral-7b-instruct:free"]' free-tier-tracker.json

# Zeige nur remaining
jq '.models | to_entries | map({key: .key, remaining: .value.remaining})' free-tier-tracker.json
```

### Logs analysieren

```bash
# MCP-Server-Logs anschauen
tail -f ~/.config/claude-code/logs/openrouter-tool.log
```

---

## üìö Weitere Ressourcen

- [FREE_TIER_README.md](./FREE_TIER_README.md) - Haupt-Dokumentation
- [OpenRouter Docs](https://openrouter.ai/docs) - API-Dokumentation
- [OpenRouter Models](https://openrouter.ai/models?max_price=0) - Free-Modelle-Liste

---

**Version:** 1.0.0
**Datum:** 2025-11-24
**Autor:** Claude Code
